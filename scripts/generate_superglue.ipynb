{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "987bf3b2-399b-42a3-bb72-158196c51184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, sys\n",
    "from pathlib import Path\n",
    "\n",
    "SESSION_NAME = \"superpoint_test\"\n",
    "\n",
    "ROOT = Path().absolute().parent\n",
    "RESULTS_PATH = Path(f\"{ROOT}/data/3_results\")\n",
    "\n",
    "SESSION_ID = SESSION_NAME if SESSION_NAME else str(int(time.time()))\n",
    "SESSION_PATH = Path(f\"{RESULTS_PATH}/{SESSION_ID}\")\n",
    "CLONE_COLMAP_PROJECT_PATH = Path(f\"{RESULTS_PATH}/3_koepenick_rathaus_SfM_werramat\")\n",
    "CLONE_COLMAP_IMAGE_PATH = f\"{CLONE_COLMAP_PROJECT_PATH}/image_path\"\n",
    "CLONE_DB_PATH = f\"{CLONE_COLMAP_PROJECT_PATH}/database.db\"\n",
    "\n",
    "if not os.path.exists(SESSION_PATH):\n",
    "    !mkdir -p {SESSION_PATH}\n",
    "    # copy images from clone path\n",
    "    !cp -r \"{CLONE_COLMAP_IMAGE_PATH}\" \"{SESSION_PATH}\"\n",
    "    # copy database from clone path\n",
    "    !cp \"{CLONE_DB_PATH}\" \"{SESSION_PATH}\"\n",
    "    print(f\"Created new session with ID {SESSION_ID} under {RESULTS_PATH}\")\n",
    "\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "\n",
    "SCRIPTS_PATH = f\"{ROOT}/third_party/SuperGluePretrainedNetwork\"\n",
    "if SCRIPTS_PATH not in sys.path:\n",
    "    sys.path.append(SCRIPTS_PATH)\n",
    "\n",
    "COLMAP_PY_SCRIPTS_PATH = f\"{ROOT}/third_party/colmap/scripts/python\"\n",
    "if COLMAP_PY_SCRIPTS_PATH not in sys.path:\n",
    "    sys.path.append(COLMAP_PY_SCRIPTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4068df9-6026-4fbb-9a42-cfa54059400f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##### Data Preparation ( raw input -> input folder)\n",
    "%cd {ROOT}\n",
    "\n",
    "raw_input_files = [x for x in RAW_INPUT_PATH.glob('**/*') if x.is_file()]\n",
    "print(f\"found {len(raw_input_files)} raw input files\")\n",
    "_f = \"/mnt/c/Users/tworkool/Documents/dev/python/historical-photo-sfm-pipeline/data/1_raw_input/lego.mp4\" #str(raw_input_files[0])\n",
    "print(_f)\n",
    "DOWNSAMPLING_RATE = 2\n",
    "\n",
    "#!bash scripts/third_party/neuralangelo/run_ffmpeg.sh {{SESSION_NAME}} {{_f}} 2\n",
    "! ffmpeg -i {_f} -vf \"select=not(mod(n\\,{DOWNSAMPLING_RATE}))\" -vsync vfr -q:v 2 {INPUT_PATH}/%06d.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f735ca3-adc6-44c1-a3ba-eed4a01f613d",
   "metadata": {},
   "source": [
    "# Reconstruction (please read)\n",
    "You have two options to continue here:\n",
    "1. Reconstruction with Colmap wrapper in Python (with pycolmap) *\n",
    "2. Reconstruction with Colmap (**PREFERRED**) **\n",
    "\n",
    "1* pycolmap only supports sparse reconstruction, unless you build colmap from source with CUDA support\n",
    "\n",
    "2** Colmap needs to be build and installed on your environment. If it is, you should use it. Dense reconstruction requires CUDA support on the machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73c08cb-86fc-4147-b6ed-551d820ab271",
   "metadata": {},
   "source": [
    "## Reconstruction with COLMAP here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ac55863-6b5c-43f3-b7be-a0cdc81b273f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cameras',), ('sqlite_sequence',), ('images',), ('keypoints',), ('descriptors',), ('matches',), ('two_view_geometries',)]\n"
     ]
    }
   ],
   "source": [
    "# reconstruction with SuperGlue\n",
    "from database import COLMAPDatabase, pair_id_to_image_ids, image_ids_to_pair_id\n",
    "import sqlite3\n",
    "\n",
    "# create db here\n",
    "DB_PATH = f\"{SESSION_PATH}/database.db\"\n",
    "colmap_db = COLMAPDatabase.connect(DB_PATH)\n",
    "cursor = colmap_db.cursor()\n",
    "\n",
    "#if os.path.exists(DB_PATH):\n",
    "#    print(\"ERROR: database path already exists -- will not modify it.\")\n",
    "#    return\n",
    "\n",
    "#colmap_db = COLMAPDatabase.connect(DB_PATH)\n",
    "#colmap_db.create_tables()\n",
    "\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91487fbf-f783-49ba-af2f-bb78ace53106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data to be read and replaced\n",
    "def name_part(full_name):\n",
    "    split_name = full_name.split(\".\")\n",
    "    if len(split_name) == 0:\n",
    "        return full_name\n",
    "    return split_name[0]\n",
    "\n",
    "def get_image_id_from_name(cursor, img_name):\n",
    "    name_only = name_part(img_name)\n",
    "    cursor.execute(f\"SELECT image_id FROM images WHERE name LIKE '{name_only}%'\")\n",
    "    image_data = cursor.fetchone()\n",
    "    if not image_data: return None\n",
    "    return image_data[0]\n",
    "\n",
    "def get_image_name_from_id(cursor, img_id):\n",
    "    cursor.execute(f\"SELECT name FROM images WHERE image_id={img_id};\")\n",
    "    image_data = cursor.fetchone()\n",
    "    if not image_data: return None\n",
    "    img_name, = image_data\n",
    "    return img_name\n",
    "    \n",
    "def generate_matching_map(cursor):\n",
    "    cursor.execute(\"SELECT pair_id FROM matches\")\n",
    "    lines = []\n",
    "    for pair_id, in cursor.fetchall():\n",
    "        img1, img2 = pair_id_to_image_ids(pair_id)\n",
    "        img1 = int(img1)\n",
    "        img2 = int(img2)\n",
    "        img1_name = get_image_name_from_id(cursor, img1)\n",
    "        img2_name = get_image_name_from_id(cursor, img2)\n",
    "        line = f\"{img1_name} {img2_name} {img1} {img2} {pair_id}\"\n",
    "        lines.append(line)\n",
    "        #print(f\"Matching {img1} / {img2}\")\n",
    "        #print(f\"{img1_name} {img2_name} OTHER PARAMS HERE\")\n",
    "    return lines\n",
    "\n",
    "def write_superglue_image_pairs_file(image_pairs, file_name):\n",
    "    with open(file_name, \"w+\") as f:\n",
    "        f.writelines(f\"{line}\\n\" for line in image_pairs)\n",
    "    return\n",
    "\n",
    "IMAGE_PAIR_FILE_NAME = \"image_pairs.txt\"\n",
    "IMAGE_PAIR_FILE_PATH = Path(f\"{SESSION_PATH}/{IMAGE_PAIR_FILE_NAME}\")\n",
    "image_pairs = generate_matching_map(cursor)\n",
    "write_superglue_image_pairs_file(image_pairs, IMAGE_PAIR_FILE_PATH)\n",
    "\n",
    "# STEPS\n",
    "# 1. SERIALIZE: generate matching_images text file for SuperGlue to use and know which images to match\n",
    "# 2. PROCESS: match images with SuperGlue\n",
    "# 3. DESERIALIZE: collect information on matches for each image pair and deserialize image pairs from the generated files generically\n",
    "# 4. POST-PROCESS/ESTIMATE: estimate which matches should be used based on \"match_confidence\"\n",
    "# 5. DB OVERWRITE\n",
    "# 5.1 Loop through all DB entries in images, matches, two_view_geometries, keypoints, ?descriptors? and overwrite the entries \n",
    "# (DB DATA SHOULD JUST BE OVERWRITTEN AND DB IS MASTER)\n",
    "# 6. Run COLMAP steps after matching!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "971bf2a4-589f-4661-b637-c1f60c4b24c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = f\"{SESSION_PATH}/SuperGlue_dump\"\n",
    "INPUT_IMAGES = f\"{SESSION_PATH}/image_path\"\n",
    "MATCH_SCRIPT_DIR = f\"{SCRIPTS_PATH}/match_pairs.py\"\n",
    "!mkdir -p {OUT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdfebc2-bbda-4070-aa17-46043e4b7535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute SuperGlue\n",
    "! python \"{MATCH_SCRIPT_DIR}\" --input_pairs \"{IMAGE_PAIR_FILE_PATH}\" --input_dir \"{INPUT_IMAGES}\" --output_dir \"{OUT_DIR}\" --resize \"-1\" --superglue outdoor --max_keypoints \"-1\" --viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2df08961-3482-40a9-8950-20b58907e032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 .npz SuperGlue dump files\n",
      "IMG_1140 IMG_1141\n",
      "IMG_1140 IMG_1142\n",
      "IMG_1140 IMG_1143\n",
      "IMG_1140 IMG_1145\n",
      "IMG_1140 IMG_1147\n",
      "IMG_1140 IMG_1148\n",
      "IMG_1140 IMG_1149\n",
      "IMG_1140 IMG_1150\n",
      "IMG_1140 IMG_1151\n",
      "IMG_1140 IMG_1160\n",
      "IMG_1140 IMG_1190\n",
      "(5896, 2)\n",
      "(4851, 2)\n",
      "[ 704. 1492.]\n",
      "(5896,)\n",
      "-1\n",
      "(5896,)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Read numpy file\n",
    "import numpy as np\n",
    "\n",
    "def get_npz_info(path):\n",
    "    npz = np.load(path)\n",
    "    #print(npz.files)\n",
    "    image_pairs = os.path.basename(path).split(\"_\")\n",
    "    img1_name = image_pairs[0] + \"_\" + image_pairs[1]\n",
    "    img2_name = image_pairs[2] + \"_\" + image_pairs[3]\n",
    "    return np.load(path), (img1_name, img2_name)\n",
    "    \n",
    "def read_superglue_dumps(dumps_directory):\n",
    "    deserialized_dumps = []\n",
    "    npz_files = []\n",
    "    for file in os.listdir(dumps_directory):\n",
    "        if file.endswith(\".npz\"):\n",
    "            abs_file_name = os.path.join(dumps_directory, file)\n",
    "            npz_files.append(abs_file_name)\n",
    "\n",
    "    print(f\"Found {len(npz_files)} .npz SuperGlue dump files\")\n",
    "    for npz_file in npz_files:\n",
    "        data, (img1_name, img2_name) = get_npz_info(npz_file)\n",
    "        print(img1_name, img2_name)\n",
    "        deserialized_dumps.append({\n",
    "            \"img1_name\": img1_name,\n",
    "            \"img2_name\": img2_name,\n",
    "            \"img1_id\": get_image_id_from_name(cursor, img1_name),\n",
    "            \"img2_id\": get_image_id_from_name(cursor, img2_name),\n",
    "            \"npz_data\": data\n",
    "        })\n",
    "    return deserialized_dumps\n",
    "\n",
    "superglue_dumps = read_superglue_dumps(OUT_DIR)\n",
    "\n",
    "def get_kp_info(kp_id, npz):\n",
    "    if kp_id >= npz['keypoints0'].shape[0]: return\n",
    "    kp = npz['keypoints0'][kp_id]\n",
    "    print(f\"KP coordinates for kp ID {kp_id} = (x {int(kp[0])}, y {int(kp[1])})\")\n",
    "    if npz['matches'][kp_id] == -1:\n",
    "        print(\"No Match!\")\n",
    "    else:\n",
    "        print(f\"Match {npz['matches'][kp_id]}! Confidence = {npz['match_confidence'][kp_id]}\")\n",
    "\n",
    "npz, _ = get_npz_info(f\"{OUT_DIR}/IMG_1140_IMG_1141_matches.npz\")\n",
    "\n",
    "print(npz['keypoints0'].shape) # kps image 1\n",
    "print(npz['keypoints1'].shape) # kps image 2\n",
    "kp_id_image1 = 5000 # id of the keypoint\n",
    "print(npz['keypoints0'][kp_id_image1])\n",
    "print(npz['matches'].shape)\n",
    "print(npz['matches'][kp_id_image1])\n",
    "print(npz['match_confidence'].shape)\n",
    "print(npz['match_confidence'][kp_id_image1])\n",
    "\n",
    "#for i in range(1, 10):\n",
    "#    get_kp_info(i, npz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c69724ab-f6e0-499d-8674-2ca124e524ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1499, 2)\n",
      "(3004, 2)\n",
      "(3267, 2)\n",
      "(1385, 2)\n",
      "(3626, 2)\n",
      "(3523, 2)\n",
      "(422, 2)\n",
      "(164, 2)\n",
      "(39, 2)\n",
      "(12, 2)\n",
      "(1456, 2)\n"
     ]
    }
   ],
   "source": [
    "def get_matches(dump, min_confidence=0.3):\n",
    "    # For each keypoint in keypoints0, the matches array indicates the index of the matching keypoint in keypoints1, or -1 if the keypoint is unmatched.\n",
    "    kp_matching_pairs = []\n",
    "    match_confidence = []\n",
    "    kp_coordinates = []\n",
    "    data = dump['npz_data']\n",
    "    for i, match in enumerate(data['matches']):\n",
    "        if match == -1:\n",
    "            continue\n",
    "        if data['match_confidence'][i] < min_confidence:\n",
    "            continue\n",
    "        # 0 = keypoints0 index, 1 = keypoints1 match pair if not -1\n",
    "        kp_matching_pairs.append(np.array([i, match]))\n",
    "        match_confidence.append(data['match_confidence'][i])\n",
    "        kp_coordinates.append(np.array([data['keypoints0'][i], data['keypoints1'][match]]))\n",
    "    return np.asarray(kp_matching_pairs), np.asarray(match_confidence), np.asarray(kp_coordinates)\n",
    "\n",
    "\n",
    "def get_keypoints_and_matches(img_id, dumps):\n",
    "    shortened_dump_entry = None\n",
    "    for d in dumps:\n",
    "        if d['img1_id'] == img_id:\n",
    "            shortened_dump_entry = {\n",
    "                \"keypoints\": d['npz_data']['keypoints0'],\n",
    "                \"matches\": d['npz_data']['matches'],\n",
    "                \"match_confidence\": d['npz_data']['match_confidence']\n",
    "            }\n",
    "            break\n",
    "        elif d['img2_id'] == img_id:\n",
    "            shortened_dump_entry = {\n",
    "                \"keypoints\": d['npz_data']['keypoints1'],\n",
    "                \"matches\": d['npz_data']['matches'],\n",
    "                \"match_confidence\": d['npz_data']['match_confidence']\n",
    "            }\n",
    "            break\n",
    "\n",
    "    return shortened_dump_entry\n",
    "\n",
    "def synchronize_db_with_dumps(cursor, dumps, validate=True):\n",
    "    num_db_matched_image_pairs = cursor.execute(\"SELECT COUNT(*) FROM matches\").fetchone()[0]\n",
    "    validation_errors = []\n",
    "    \n",
    "    # validate that number of image pairs matched in DB is the same as in dumps!\n",
    "    if num_db_matched_image_pairs != len(dumps):\n",
    "        validation_errors.append(\"cannot synchronize DB image matching pairs with dumps\")\n",
    "\n",
    "    # validate number of keypoints matches for all reoccuring images!\n",
    "    keypoint_map = {}\n",
    "    for d in dumps:\n",
    "        npz_data = d['npz_data']\n",
    "        img1_id = d['img1_id']\n",
    "        img2_id = d['img2_id']\n",
    "        img1_kp_num = npz_data['keypoints0'].shape[0]\n",
    "        img2_kp_num = npz_data['keypoints1'].shape[0]\n",
    "        if img1_id not in keypoint_map:\n",
    "            keypoint_map[img1_id] = {\n",
    "                \"num_kps\": img1_kp_num,\n",
    "                \"data\": {\n",
    "                    \"keypoints\": npz_data['keypoints0'],\n",
    "                    \"matches\": npz_data['matches'],\n",
    "                    \"match_confidence\": npz_data['match_confidence']\n",
    "                }\n",
    "            }\n",
    "        elif keypoint_map[img1_id]['num_kps'] != img1_kp_num:\n",
    "            validation_errors.append(f\"ERROR: keypoint mismatch for image {d['img1_name']}: {keypoint_map[img1_id]['num_kps']} != {img1_kp_num}\")\n",
    "\n",
    "        if img2_id not in keypoint_map:\n",
    "            keypoint_map[img2_id] = {\n",
    "                \"num_kps\": img2_kp_num,\n",
    "                \"data\": {\n",
    "                    \"keypoints\": npz_data['keypoints1'],\n",
    "                    \"matches\": npz_data['matches'],\n",
    "                    \"match_confidence\": npz_data['match_confidence']\n",
    "                }\n",
    "            }\n",
    "        elif keypoint_map[img2_id]['num_kps'] != img2_kp_num:\n",
    "            validation_errors.append(f\"ERROR: keypoint mismatch for image {d['img2_name']}: {keypoint_map[img2_id]['num_kps']} != {img2_kp_num}\")\n",
    "    \n",
    "    if validate and len(validation_errors) > 0:\n",
    "        print(\"ERROR! Validation errors:\")\n",
    "        for i, v_error in enumerate(validation_errors):\n",
    "            print(f\"{i+1}.\\t {v_error}\")\n",
    "        print(\"Aborting synchronization due to errors\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    cursor.execute(\"DELETE FROM keypoints\")\n",
    "    cursor.execute(\"DELETE FROM matches\")\n",
    "    cursor.execute(\"DELETE FROM two_view_geometries\")\n",
    "    colmap_db.commit()\n",
    "    \n",
    "    # write keypoints\n",
    "    for image_id, img_info in keypoint_map.items():\n",
    "        data = img_info['data']\n",
    "        colmap_db.add_keypoints(image_id, data['keypoints'])\n",
    "        #cursor.execute(\n",
    "        #    \"INSERT INTO keypoints VALUES (?, ?, ?, ?)\",\n",
    "        #    (image_id,) + data['keypoints'].shape + (array_to_blob(data['keypoints']),),\n",
    "        #)\n",
    "\n",
    "    for d in dumps:\n",
    "        m_idx, m_c, kp_coords = get_matches(d)\n",
    "        print(m_idx.shape)\n",
    "        colmap_db.add_matches(d['img1_id'], d['img2_id'], m_idx)\n",
    "        tv_m_idx, tv_m_c, tv_kp_coords = get_matches(d, 0.45)\n",
    "        colmap_db.add_two_view_geometry(d['img1_id'], d['img2_id'], tv_m_idx)\n",
    "\n",
    "    colmap_db.commit()\n",
    "    return\n",
    "\n",
    "synchronize_db_with_dumps(cursor, superglue_dumps, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c602b4-3018-4ec6-a3d3-4d9122b76fc2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sparse Reconstruction (Step 1)\n",
    "! bash run_colmap_sparse.sh \"{SESSION_PATH}\" \"{INPUT_PATH}\"\n",
    "#MASK_PATH = f\"{INPUT_PATH}/masks\"\n",
    "#! bash run_colmap_sparse_with_masks.sh \"{SESSION_PATH}\" \"{INPUT_PATH}\" \"{MASK_PATH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ab1663-26de-4861-a14b-733effa99f80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dense Reconstruction IF NEEDED (Step 2)\n",
    "! bash run_colmap_dense.sh \"{SESSION_PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a594876f-94b2-49b2-ba22-89629b86d950",
   "metadata": {},
   "source": [
    "## Generate Transforms File\n",
    "Generate this file if you want to have camera, points etc. in a self contained file which you can then use to display everything in a 3D engine like Blender (see my scripts `colmap_pc_importer_ui.py` and `colmap_pc_importer.py` which u can load into Blender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13695c5-1659-4b9e-b74b-2f2192442a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "\n",
    "print(sys.path)\n",
    "\n",
    "generate_transforms_script = f\"{ROOT}/scripts/third_party/neuralangelo/convert_data_to_json_advanced.py\"\n",
    "! python {generate_transforms_script} --data_dir \"{SESSION_PATH}\" --scene_type \"outdoor\" --image_dir \"{INPUT_PATH}\"\n",
    "\n",
    "#generate_transforms_script = f\"{ROOT}/scripts/third_party/neuralangelo/convert_data_to_json.py\"\n",
    "#! python {generate_transforms_script} --data_dir \"{SESSION_PATH}\" --scene_type \"outdoor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ceae5f-15ad-457f-9479-acdc38f0f077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL.ExifTags import TAGS, GPSTAGS\n",
    "\n",
    "def get_exif_data(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        exif_data = image._getexif()\n",
    "\n",
    "        if exif_data is not None:\n",
    "            # Decode the EXIF data\n",
    "            decoded_exif = {TAGS[key]: exif_data[key] for key in exif_data.keys() if key in TAGS and isinstance(exif_data[key], (int, str, bytes))}\n",
    "            return decoded_exif\n",
    "        else:\n",
    "            print(\"No EXIF data found.\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading EXIF data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "image_path = '/mnt/d/dev/python/historical-photo-sfm-pipeline/data/2_input/IMG_0555.jpeg'\n",
    "exif_data = get_exif_data(image_path)\n",
    "\n",
    "if exif_data:\n",
    "    print(\"EXIF Metadata:\")\n",
    "    for key, value in exif_data.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d30ad0-e9a4-469e-88b2-7763eb4da2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "camera = pycolmap.Camera(\n",
    "    model=camera_model_name_or_id,\n",
    "    width=width,\n",
    "    height=height,\n",
    "    params=params,\n",
    ")\n",
    "\n",
    "import pycolmap\n",
    "reconstruction = pycolmap.Reconstruction(\"path/to/reconstruction/dir\")\n",
    "print(reconstruction.summary())\n",
    "\n",
    "for image_id, image in reconstruction.images.items():\n",
    "    print(image_id, image)\n",
    "\n",
    "for point3D_id, point3D in reconstruction.points3D.items():\n",
    "    print(point3D_id, point3D)\n",
    "\n",
    "for camera_id, camera in reconstruction.cameras.items():\n",
    "    print(camera_id, camera)\n",
    "\n",
    "reconstruction.write(\"path/to/reconstruction/dir/\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
